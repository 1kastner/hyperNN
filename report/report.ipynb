{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HyperNN : Neural network's hyperparameters optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are very popular nowadays, and a lot of libraries propose very efficient way of using them. A lot of problems are solvable using a neural network but finding the right one, the more efficient, the more accurate is very complex and running a single model can take a long time. Them finding the optimal parameters of a neural network can be difficult. People are used to optmise them by hand, using their knowledge on neural networks. The effort required to find an optimal or quasi optimal configuration is so high that people usually use non-optmized netwoks. In this report we compare a few solutions proposed to optimize these parameters. We applyed the solutions on the dataset MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "present the implemented articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we used the well known dataset MNIST. The dataset is composed by 80 000 classified images reprensenting the numbers from 0 to 9. \n",
    "The training set is composed of 60 000 images, the test set and the validation set 10 000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to implement the model using keras (python library). Using tensorflow or theano to encode neural networks. We choose to use tensorflow for this project.<br>\n",
    "The basic model is a logistic regression with 784 inputs and 10 outputs using as loss the categorical crossentropy and as optimizer a SGD. \n",
    "We choose to fix the number of epochs to 100 and the batch size to 200. This choice was made to simplify the problem and to impose the network to find a solution in a reasonable time.<br>\n",
    "\n",
    "\n",
    "All remaning parameters are used for the optimisation.<br>\n",
    "\n",
    "One MNIST training took 1-2 minutes on our computers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose for this project to optimise the network on the following parameters : <br>\n",
    "- number of layers : from 0 to 3 <br>\n",
    "- number of neurons per layer : from 10 to 500 <br>\n",
    "- learning rate : from 0.001 to 0.8  <br>\n",
    "- L1 regularisation : from 1e-6 to 0.1  <br>\n",
    "- L2 regularisation : from 1e-6 to 0.1  <br>\n",
    "- moment : from 0 to 0.8<br>\n",
    "- decay : from 0 to 0.8<br>\n",
    "- nesterov : true or false<br>\n",
    "- activation function : tanh or sigmoid or relu<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperRand : random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach we've tried was to make a random search on the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperLearn : NN desning NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this approach we've re-implemented the solution proposed by ............... in the paper ............. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of NN designing NN is to learn a neural network that tries to predict the performance of a Network givens its parameters. The complicated point of this approach is to generate some points on which the neural network can learn, then to refine the neural network on interessant zones to find the optimal solution. The sampling of training points is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample some random hyperparameters and train MNIST on them\n",
    "train the neural network (RSM)\n",
    "a = 1e-4\n",
    "while (n_iteration < max_iter):\n",
    "    sample a random parameter\n",
    "    predict its performance p\n",
    "    if (p> max_performance):\n",
    "        with probability (1-a):\n",
    "            train_MNIST\n",
    "            add result in RSM training set\n",
    "            train the RSM\n",
    "            update max_performance if necessary\n",
    "        else do nothing\n",
    "    else :\n",
    "        with probability a\n",
    "            train_MNIST\n",
    "            add result in RSM training set\n",
    "            train the RSM\n",
    "            update max_performance if necessary\n",
    "        else do nothing\n",
    "return the best configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous algorithm is very dependant on sampling hyperparameters to test. The sampling method is a variation of Metropolisâ€“Hastings algorithm. The first sample are random, the following are sampled with a gaussian distribution around the previous one. This method enable a local search around the optimal solution found, but also allows the exploration of the rest of the search space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've beginned using when possible continous hyperparameters values. But this approach was stuck is a local minima and was not able to explore the entire space to find a good solution. This bad result is understandable because of the meaning of the values : a learning rate of 0.1 is very similar to a leaning rate of 0.13, but a learning rate of 0.001 is quite different of a learning rate of 0.031. The gaussian distribution doesn't make any difference between these two examples. Then we followed what was done in the article and we discretized the search space. This way we were able to counter the previous problem, but we've lost in the precision of the optained solution. <br>\n",
    "Here are the possible values of the parameters : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = np.array([[0, 1, 2, 3], #n_couches\n",
    "                range(10, 500, 10), range(10, 500, 10), range(10, 500, 10), #couches\n",
    "                [0.001, 0.002, 0.004, 0.008, 0.016, 0.03, 0.06, 0.012, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8], #learning rate\n",
    "                [0.000001,0.00001,0.0001,0.001,0.01,0.1], #reg_l1\n",
    "                [0.000001,0.00001,0.0001,0.001,0.01,0.1], #reg_l2\n",
    "                [0.001, 0.002, 0.004, 0.008, 0.016, 0.03, 0.06, 0.012, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8], #moment\n",
    "                [.0,0.001, 0.002, 0.004, 0.008, 0.016, 0.03, 0.06, 0.012, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8], #decay\n",
    "                [0,1], #nesterov\n",
    "                [0, 1, 2] #activation\n",
    "                ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}